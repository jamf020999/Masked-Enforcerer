{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ce65b93b",
      "metadata": {
        "id": "ce65b93b"
      },
      "source": [
        "# Masked Enforcerer – Face Mask Detection with YOLOv8\n",
        "\n",
        "Author: **Jose Moreno**  \n",
        "Course: **ITAI 1378 – Computer Vision & AI**  \n",
        "Tier 1 Core Project: **Automated Mask Compliance Detection**\n",
        "\n",
        "This notebook implements the Tier‑1 project described in the *Masked Enforcerer* proposal. It trains a YOLOv8 object detection model to classify people as **with mask**, **without mask**, or **mask worn incorrectly**, and then lets the user **upload three images** to test the trained model.\n",
        "\n",
        "**High‑level pipeline:**\n",
        "1. Set up the environment (PyTorch + Ultralytics YOLOv8).\n",
        "2. Download / upload a public *Face Mask Detection* dataset (Kaggle, 3‑class).\n",
        "3. Convert PASCAL VOC XML annotations to YOLO format.\n",
        "4. Split data into **train (70%)**, **val (15%)**, **test (15%)**.\n",
        "5. Train a **YOLOv8n** model (fast & light) on the dataset.\n",
        "6. Evaluate basic metrics (YOLO’s training summary).\n",
        "7. Let the user **upload three images** and run inference to label mask usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad412b9b",
      "metadata": {
        "id": "ad412b9b"
      },
      "source": [
        "## 1. Environment setup\n",
        "\n",
        "This notebook was created thanks to the settings and tools available on **Google Colab**, but it will also work in a local Jupyter environment as long as there is installed on the machine a GPU‑enabled PyTorch + Ultralytics stack. Following this, the environment setup is initialized to serve as foundations for the project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "299811ec",
      "metadata": {
        "id": "299811ec"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics kaggle gdown\n",
        "\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "\n",
        "print('PyTorch version:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU name:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Running on CPU – training will be slower.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108595a9",
      "metadata": {
        "id": "108595a9"
      },
      "source": [
        "## 2. Dataset: 3‑class Face Mask Detection (public)\n",
        "\n",
        "For the final project I have decided to work with the public **Face Mask Detection** dataset (Kaggle, by `andrewmvd`), which contains:\n",
        "- **853 images**\n",
        "- **3 classes**: `with_mask`, `without_mask`, `mask_weared_incorrect`\n",
        "- **PASCAL VOC** XML annotations with bounding boxes for each face\n",
        "\n",
        "Because the Kaggle API requires ownership of credentials, this notebook supports **two workflows** were explored:\n",
        "\n",
        "### Option A – Download automatically via Kaggle API (recommended)\n",
        "1. Go to Kaggle → Account → *Create New API Token* → download `kaggle.json`.\n",
        "2. Upload `kaggle.json` to `/content`.\n",
        "3. Run the cell below (it will install Kaggle CLI, move the token, and download the dataset).\n",
        "-----------------------------------------------------\n",
        "### Option B – Manual upload\n",
        "Manually download the zip file containing the dataset from the Kaggle API and then uploading it on the editor, in this case colab\n",
        "\n",
        "-----------------------------------------------------\n",
        "\n",
        "It was later decided that to make the project more accessible to the public, that the best approach would be to upload the dataset in google drive instead of manually uploading it on colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ea423c88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea423c88",
        "outputId": "6522cf0b-c4e0-4e9d-ec0f-2847e99d6435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Google Drive dataset download automatically...\n",
            "Downloading dataset zip from Google Drive...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1peUij_PsdCdWfE5GV6ZPi1i6tH7Wu1qG\n",
            "From (redirected): https://drive.google.com/uc?id=1peUij_PsdCdWfE5GV6ZPi1i6tH7Wu1qG&confirm=t&uuid=01f58438-6578-4272-89d4-7be837aac006\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 417M/417M [00:02<00:00, 140MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download complete. Saved to /content/archive.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os, zipfile, pathlib, gdown\n",
        "\n",
        "DATASET_DIR = pathlib.Path('face-mask-detection')\n",
        "DATA_ZIP_PATH = pathlib.Path('archive.zip')\n",
        "\n",
        "# File ID from your shared Google Drive link (already public)\n",
        "DRIVE_FILE_ID = \"1peUij_PsdCdWfE5GV6ZPi1i6tH7Wu1qG\"\n",
        "\n",
        "def download_from_gdrive():\n",
        "    \"\"\"Download the face-mask-detection dataset ZIP from Google Drive (public link).\n",
        "\n",
        "    This will download 'archive.zip' and save it in the current working directory.\n",
        "    \"\"\"\n",
        "    url = f\"https://drive.google.com/uc?id={DRIVE_FILE_ID}\"\n",
        "    print(\"Downloading dataset zip from Google Drive...\")\n",
        "    gdown.download(url, str(DATA_ZIP_PATH), quiet=False)\n",
        "    print(\"Download complete. Saved to\", DATA_ZIP_PATH.resolve())\n",
        "\n",
        "print(\"Starting Google Drive dataset download automatically...\")\n",
        "download_from_gdrive()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40778250",
      "metadata": {
        "id": "40778250"
      },
      "source": [
        "## 3. Unzip dataset and inspect structure\n",
        "\n",
        "This cell looks for a **`.zip`** file in the current directory (for example, the Kaggle download) and\n",
        "extracts it into a `face-mask-detection/` folder that contains `images/` and `annotations/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ff06da65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff06da65",
        "outputId": "1240f50a-ff0b-4d16-ce41-15afac24b2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using zip file: archive.zip\n",
            "Extracted dataset to: /content/face-mask-detection\n",
            "face-mask-detection/\n",
            "  annotations/\n",
            "    maksssksksss356.xml\n",
            "    maksssksksss33.xml\n",
            "    maksssksksss722.xml\n",
            "    maksssksksss484.xml\n",
            "    maksssksksss833.xml\n",
            "  images/\n",
            "    maksssksksss490.png\n",
            "    maksssksksss256.png\n",
            "    maksssksksss402.png\n",
            "    maksssksksss405.png\n",
            "    maksssksksss3.png\n"
          ]
        }
      ],
      "source": [
        "import os, zipfile, glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Find any zip file if the dataset dir does not already exist\n",
        "if not DATASET_DIR.exists():\n",
        "    zip_candidates = [f for f in os.listdir('.') if f.lower().endswith('.zip')]\n",
        "    if not zip_candidates:\n",
        "        raise FileNotFoundError(\n",
        "            'No .zip file found. Either run download_with_kaggle() first or manually upload the Kaggle dataset zip.'\n",
        "        )\n",
        "\n",
        "    zip_path = zip_candidates[0]\n",
        "    print('Using zip file:', zip_path)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(DATASET_DIR)\n",
        "\n",
        "    print('Extracted dataset to:', DATASET_DIR.resolve())\n",
        "else:\n",
        "    print('Dataset directory already exists at', DATASET_DIR.resolve())\n",
        "\n",
        "# Inspect basic structure\n",
        "for root, dirs, files in os.walk(DATASET_DIR):\n",
        "    level = root.replace(str(DATASET_DIR), '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for f in files[:5]:  # show only a few files per directory\n",
        "        print(f\"{subindent}{f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d727208f",
      "metadata": {
        "id": "d727208f"
      },
      "source": [
        "## 4. Convert PASCAL VOC XML → YOLO format + Train/Val/Test split\n",
        "\n",
        "The Kaggle dataset annotations are in **PASCAL VOC** XML format. YOLOv8 expects one `.txt` file per image, with each line containing:\n",
        "\n",
        "```text\n",
        "<class_id> <cx> <cy> <w> <h>\n",
        "```\n",
        "\n",
        "All coordinates are **normalized** to `[0, 1]` relative to the image width/height.\n",
        "\n",
        "In this step the model will:\n",
        "1. Map classes to indices:\n",
        "   - `with_mask` → **0**\n",
        "   - `without_mask` → **1**\n",
        "   - `mask_weared_incorrect` → **2**\n",
        "2. Randomly split the images into **train (70%)**, **val (15%)**, **test (15%)**.\n",
        "3. Copy images into `images/{train,val,test}/` and create labels in `labels/{train,val,test}/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "322deaec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "322deaec",
        "outputId": "5ef169bf-91ea-49a4-c70f-c90f49258dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotations dir: face-mask-detection/annotations\n",
            "Images dir     : face-mask-detection/images\n",
            "Total images found: 853\n",
            "Train: 597  Val: 127  Test: 129\n",
            "VOC → YOLO conversion complete.\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "from pathlib import Path\n",
        "import random, shutil\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "annotations_dir = next(DATASET_DIR.glob('**/annotations'))\n",
        "images_dir = next(DATASET_DIR.glob('**/images'))\n",
        "\n",
        "print('Annotations dir:', annotations_dir)\n",
        "print('Images dir     :', images_dir)\n",
        "\n",
        "# Class mapping for YOLO\n",
        "CLASS_MAP = {\n",
        "    'with_mask': 0,\n",
        "    'without_mask': 1,\n",
        "    'mask_weared_incorrect': 2\n",
        "}\n",
        "\n",
        "image_paths = sorted(list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpg')))\n",
        "print('Total images found:', len(image_paths))\n",
        "\n",
        "random.shuffle(image_paths)\n",
        "n_total = len(image_paths)\n",
        "n_train = int(0.70 * n_total)\n",
        "n_val = int(0.15 * n_total)\n",
        "n_test = n_total - n_train - n_val\n",
        "\n",
        "splits = {\n",
        "    'train': image_paths[:n_train],\n",
        "    'val': image_paths[n_train:n_train + n_val],\n",
        "    'test': image_paths[n_train + n_val:]\n",
        "}\n",
        "\n",
        "print(f'Train: {len(splits[\"train\"])}  Val: {len(splits[\"val\"])}  Test: {len(splits[\"test\"])}')\n",
        "\n",
        "for split, paths in splits.items():\n",
        "    img_out_dir = DATASET_DIR / 'images' / split\n",
        "    lbl_out_dir = DATASET_DIR / 'labels' / split\n",
        "    img_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    lbl_out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for img_path in paths:\n",
        "        # Copy image\n",
        "        shutil.copy2(img_path, img_out_dir / img_path.name)\n",
        "\n",
        "        # Build XML path and parse\n",
        "        xml_path = annotations_dir / f\"{img_path.stem}.xml\"\n",
        "        if not xml_path.exists():\n",
        "            print('Warning: missing annotation for', img_path.name)\n",
        "            continue\n",
        "\n",
        "        tree = ET.parse(str(xml_path))\n",
        "        root_xml = tree.getroot()\n",
        "        w = float(root_xml.find('./size/width').text)\n",
        "        h = float(root_xml.find('./size/height').text)\n",
        "\n",
        "        yolo_lines = []\n",
        "        for obj in root_xml.findall('object'):\n",
        "            cls_name = obj.find('name').text\n",
        "            if cls_name not in CLASS_MAP:\n",
        "                continue\n",
        "            cls_id = CLASS_MAP[cls_name]\n",
        "\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = float(bbox.find('xmin').text)\n",
        "            ymin = float(bbox.find('ymin').text)\n",
        "            xmax = float(bbox.find('xmax').text)\n",
        "            ymax = float(bbox.find('ymax').text)\n",
        "\n",
        "            # Convert to YOLO (normalized center x,y + width/height)\n",
        "            cx = (xmin + xmax) / 2.0 / w\n",
        "            cy = (ymin + ymax) / 2.0 / h\n",
        "            bw = (xmax - xmin) / w\n",
        "            bh = (ymax - ymin) / h\n",
        "\n",
        "            yolo_lines.append(f\"{cls_id} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
        "\n",
        "        label_path = lbl_out_dir / f\"{img_path.stem}.txt\"\n",
        "        with open(label_path, 'w') as f:\n",
        "            f.write('\\n'.join(yolo_lines))\n",
        "\n",
        "print('VOC → YOLO conversion complete.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67cd43d8",
      "metadata": {
        "id": "67cd43d8"
      },
      "source": [
        "## 5. Create YOLOv8 dataset config (`face_mask.yaml`)\n",
        "\n",
        "YOLOv8 uses a small YAML file to describe where the images and labels live, and what the class names are.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "51add3aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51add3aa",
        "outputId": "a92fd162-e931-4fa2-903a-e123336e3fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote YOLO dataset config to /content/face_mask.yaml\n",
            "\n",
            "=== face_mask.yaml ===\n",
            "path: /content/face-mask-detection\n",
            "train: images/train\n",
            "val: images/val\n",
            "test: images/test\n",
            "\n",
            "names:\n",
            "  0: with_mask\n",
            "  1: without_mask\n",
            "  2: mask_weared_incorrect\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data_yaml_path = Path('face_mask.yaml')\n",
        "\n",
        "data_yaml_content = f\"\"\"\n",
        "path: {DATASET_DIR.resolve()}\n",
        "train: images/train\n",
        "val: images/val\n",
        "test: images/test\n",
        "\n",
        "names:\n",
        "  0: with_mask\n",
        "  1: without_mask\n",
        "  2: mask_weared_incorrect\n",
        "\"\"\".strip() + \"\\n\"\n",
        "\n",
        "with open(data_yaml_path, 'w') as f:\n",
        "    f.write(data_yaml_content)\n",
        "\n",
        "print('Wrote YOLO dataset config to', data_yaml_path.resolve())\n",
        "print('\\n=== face_mask.yaml ===')\n",
        "print(data_yaml_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1e3599",
      "metadata": {
        "id": "ad1e3599"
      },
      "source": [
        "## 6. Train YOLOv8n on the face mask dataset\n",
        "\n",
        "Working with a pre‑trained YOLOv8n model (small, fast) on the face mask dataset. This is were the training proper will begin, with a set number of epochs which will determine how many times the dataset will be scanned.\n",
        "\n",
        "You can adjust hyperparameters (epochs, image size, batch size) as needed. For Colab Free, **20–30 epochs** is usually a good balance between speed and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ad19568c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad19568c",
        "outputId": "7694e484-2c6e-483a-9306-8bb7b7f9d10a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 99.6MB/s 0.1s\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 26.5MB/s 0.0s\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 120.7MB/s 0.0s\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/face-mask-detection/labels/train... 597 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 597/597 889.7it/s 0.7s\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/face-mask-detection/labels/val... 127 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 127/127 438.5it/s 0.3s\n",
            "\u001b[K       1/10      2.76G        1.8      3.005      1.458         23        640: 100% ━━━━━━━━━━━━ 38/38 2.1it/s 17.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.0s/it 4.0s\n",
            "\u001b[K       2/10      2.77G      1.339      1.638      1.092         75        640: 100% ━━━━━━━━━━━━ 38/38 3.1it/s 12.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "\u001b[K       3/10      2.79G      1.323      1.431      1.073         24        640: 100% ━━━━━━━━━━━━ 38/38 2.8it/s 13.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "\u001b[K       4/10      2.82G      1.271      1.286      1.048         20        640: 100% ━━━━━━━━━━━━ 38/38 2.6it/s 14.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.8it/s 2.2s\n",
            "\u001b[K       5/10      2.82G      1.209      1.167      1.033         26        640: 100% ━━━━━━━━━━━━ 38/38 2.9it/s 13.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.9it/s 1.4s\n",
            "\u001b[K       6/10      2.85G      1.186      1.081      1.024         15        640: 100% ━━━━━━━━━━━━ 38/38 2.8it/s 13.6s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.8it/s 1.4s\n",
            "\u001b[K       7/10      2.86G      1.144     0.9847      1.004         17        640: 100% ━━━━━━━━━━━━ 38/38 2.9it/s 13.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.5it/s 1.6s\n",
            "\u001b[K       8/10      2.88G      1.141     0.9217     0.9958         24        640: 100% ━━━━━━━━━━━━ 38/38 2.9it/s 13.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.6it/s 2.5s\n",
            "\u001b[K       9/10      2.89G       1.11     0.8737     0.9846         89        640: 100% ━━━━━━━━━━━━ 38/38 2.9it/s 13.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.6it/s 1.6s\n",
            "\u001b[K      10/10      2.92G      1.056      0.824     0.9676         23        640: 100% ━━━━━━━━━━━━ 38/38 2.9it/s 13.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 2.4it/s 1.7s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 4/4 1.1it/s 3.5s\n",
            "Training complete. Best weights saved to runs/mask_yolov8n/weights/best.pt\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load pre-trained YOLOv8n model\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "results = model.train(\n",
        "    data='face_mask.yaml',\n",
        "    epochs=10,\n",
        "    imgsz=640,\n",
        "    batch=16,\n",
        "    patience=10,\n",
        "    verbose=True,\n",
        "    project='runs',\n",
        "    name='mask_yolov8n',\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "print('Training complete. Best weights saved to runs/mask_yolov8n/weights/best.pt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3160a015",
      "metadata": {
        "id": "3160a015"
      },
      "source": [
        "## 7. Quick evaluation summary\n",
        "\n",
        "YOLOv8 logs metrics like **mAP**, **precision**, and **recall** during training. In Colab, you can open the `runs/mask_yolov8n` folder to inspect:\n",
        "- `results.png` – overall training curves (loss, mAP, precision, recall).\n",
        "- `confusion_matrix.png` – class confusion.\n",
        "- `PR_curve.png` – precision–recall curve.\n",
        "\n",
        "The cell below just prints where to find these artifacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9175fa88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9175fa88",
        "outputId": "f3aa920b-2819-4651-dec6-12e2a85302e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training artifacts located at: /content/runs/mask_yolov8n\n",
            "Check this folder for results.png, confusion_matrix.png, PR_curve.png, etc.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "run_dir = Path('runs') / 'mask_yolov8n'\n",
        "print('Training artifacts located at:', run_dir.resolve())\n",
        "print('Check this folder for results.png, confusion_matrix.png, PR_curve.png, etc.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6db4ffa7",
      "metadata": {
        "id": "6db4ffa7"
      },
      "source": [
        "## 8. Inference: upload three images and detect mask usage\n",
        "\n",
        "**Important: if there are no sample images available, refer to the main page of the repository and click on the folder named \"stock\" then use any of the images contained on that folder to proceed with the test.**\n",
        "\n",
        "Now that the model is trained, you can **upload any three images** containing people’s faces. The model will:\n",
        "- Detect faces.\n",
        "- Classify each detection as `with_mask`, `without_mask`, or `mask_weared_incorrect`.\n",
        "- Save annotated images to a new `runs/detect/` folder.\n",
        "\n",
        "**Instructions:**\n",
        "1. Run the cell below.\n",
        "2. Use the file picker to upload up to three images from your computer.\n",
        "3. After inference, open the `runs/detect/` folder in the Colab file browser to view the labeled outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb6c3979",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "fb6c3979",
        "outputId": "f46f5ff8-760f-4978-c8a4-6bee7b17f7c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please select up to three images (JPG/PNG)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-37fc9066-3b6c-41af-8ffb-e81f125e26e9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-37fc9066-3b6c-41af-8ffb-e81f125e26e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 106467352-1585602933667virus-medical-flu-mask-health-protection-woman-young-outdoor-sick-pollution-protective-danger-face_t20_o07dbe.jpg to 106467352-1585602933667virus-medical-flu-mask-health-protection-woman-young-outdoor-sick-pollution-protective-danger-face_t20_o07dbe.jpg\n",
            "Saving images (8).jpeg to images (8).jpeg\n",
            "Saving istockphoto-1044252330-612x612.jpg to istockphoto-1044252330-612x612.jpg\n",
            "Running inference on: ['106467352-1585602933667virus-medical-flu-mask-health-protection-woman-young-outdoor-sick-pollution-protective-danger-face_t20_o07dbe.jpg', 'images (8).jpeg', 'istockphoto-1044252330-612x612.jpg']\n",
            "Inference complete. Check the latest folder inside runs/detect/ for annotated images.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Load best model weights from training\n",
        "inference_model = YOLO('runs/mask_yolov8n/weights/best.pt')\n",
        "\n",
        "print('Please select up to three images (JPG/PNG)...')\n",
        "uploaded = files.upload()\n",
        "\n",
        "image_paths = list(uploaded.keys())\n",
        "print('Running inference on:', image_paths)\n",
        "\n",
        "results = inference_model.predict(\n",
        "    source=image_paths,\n",
        "    conf=0.35,\n",
        "    imgsz=640,\n",
        "    save=True\n",
        ")\n",
        "\n",
        "print('Inference complete. Check the latest folder inside runs/detect/ for annotated images.')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics.utils import ops\n",
        "\n",
        "# Class names in the correct order:\n",
        "class_names = [\"with_mask\", \"without_mask\", \"mask_weared_incorrect\"]\n",
        "\n",
        "print(\"DETECTION RESULTS:\\n\")\n",
        "\n",
        "for i, r in enumerate(results):\n",
        "    print(f\"Image {i+1}: {r.path}\")\n",
        "\n",
        "    boxes = r.boxes\n",
        "    if boxes is None or len(boxes) == 0:\n",
        "        print(\"  No detections.\\n\")\n",
        "        continue\n",
        "\n",
        "    for b in boxes:\n",
        "        cls_id = int(b.cls[0])\n",
        "        conf = float(b.conf[0])\n",
        "        label = class_names[cls_id]\n",
        "\n",
        "        print(f\"  → {label}  (confidence: {conf:.2f})\")\n",
        "\n",
        "    print()  # blank line between images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-pliHVTMyuu",
        "outputId": "f41f82ad-5acc-42a6-893b-863f01be3853"
      },
      "id": "k-pliHVTMyuu",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DETECTION RESULTS:\n",
            "\n",
            "Image 1: 106467352-1585602933667virus-medical-flu-mask-health-protection-woman-young-outdoor-sick-pollution-protective-danger-face_t20_o07dbe.jpg\n",
            "  → with_mask  (confidence: 0.95)\n",
            "\n",
            "Image 2: images (8).jpeg\n",
            "  → with_mask  (confidence: 0.74)\n",
            "\n",
            "Image 3: istockphoto-1044252330-612x612.jpg\n",
            "  → without_mask  (confidence: 0.76)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871afdf0",
      "metadata": {
        "id": "871afdf0"
      },
      "source": [
        "## 9. Optional: batch evaluation on the held‑out test set\n",
        "\n",
        "This optional cell runs inference on the **test split** and writes predictions and labels into a `runs/detect/mask_test/` folder. You can use this for more detailed error analysis if desired. This it not necessary for the project to be functional however given that this evaluates the entire dataset and serves the purpose of test evaluation its better to use this to evaluate the model as a whole.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6cf09061",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cf09061",
        "outputId": "c31eab76-c5cf-4db2-cec9-6ab4ad37fefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test images dir: face-mask-detection/images/test\n",
            "Saved test predictions to runs/mask_test\n"
          ]
        }
      ],
      "source": [
        "test_images_dir = DATASET_DIR / 'images' / 'test'\n",
        "print('Test images dir:', test_images_dir)\n",
        "\n",
        "test_results = inference_model.predict(\n",
        "    source=str(test_images_dir),\n",
        "    conf=0.35,\n",
        "    imgsz=640,\n",
        "    save=True,\n",
        "    project='runs',\n",
        "    name='mask_test',\n",
        "    exist_ok=True\n",
        "    )\n",
        "print('Saved test predictions to runs/mask_test')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sl8KYbxCvVpv"
      },
      "id": "Sl8KYbxCvVpv",
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}